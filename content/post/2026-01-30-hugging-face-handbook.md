+++
date = '2026-01-30T11:53:30+08:00'
draft = false
title = 'Hugging Face é€ŸæŸ¥æ‰‹å†Œ'
image = '/images/bg/switzerland-alps.jpg'
categories = ['AI', 'æ·±åº¦å­¦ä¹ ', 'Hugging Face']
tags = ['AI', 'æ·±åº¦å­¦ä¹ ', 'Hugging Face', 'å¤§æ¨¡å‹']
+++

# Hugging Face é€ŸæŸ¥æ‰‹å†Œ

## ç›®å½•

- [1. å®‰è£…](#1-å®‰è£…)
- [2. Pipeline å¿«é€Ÿå…¥é—¨](#2-pipeline-å¿«é€Ÿå…¥é—¨)
- [3. Tokenizer åˆ†è¯å™¨](#3-tokenizer-åˆ†è¯å™¨)
- [4. æ¨¡å‹åŠ è½½ä¸ä½¿ç”¨](#4-æ¨¡å‹åŠ è½½ä¸ä½¿ç”¨)
- [5. Datasets æ•°æ®é›†](#5-datasets-æ•°æ®é›†)
- [6. æ¨¡å‹è®­ç»ƒä¸å¾®è°ƒ](#6-æ¨¡å‹è®­ç»ƒä¸å¾®è°ƒ)
- [7. æ¨¡å‹ä¿å­˜ä¸ä¸Šä¼ ](#7-æ¨¡å‹ä¿å­˜ä¸ä¸Šä¼ )
- [8. å¸¸ç”¨æ¨¡å‹é€ŸæŸ¥](#8-å¸¸ç”¨æ¨¡å‹é€ŸæŸ¥)
- [9. é«˜çº§é…ç½®](#9-é«˜çº§é…ç½®)
- [10. å¸¸è§é—®é¢˜è§£å†³](#10-å¸¸è§é—®é¢˜è§£å†³)

---

## 1. å®‰è£…

### åŸºç¡€å®‰è£…

```bash
# å®‰è£… transformers
pip install transformers

# å®‰è£…å…¨å¥—å·¥å…·
pip install transformers datasets evaluate accelerate

# å®‰è£… PyTorch ç‰ˆæœ¬
pip install transformers[torch]

# å®‰è£… TensorFlow ç‰ˆæœ¬
pip install transformers[tf-cpu]

# ä»æºç å®‰è£…ï¼ˆæœ€æ–°ç‰ˆï¼‰
pip install git+https://github.com/huggingface/transformers
```

### éªŒè¯å®‰è£…

```python
import transformers
print(transformers.__version__)
```

---

## 2. Pipeline å¿«é€Ÿå…¥é—¨

Pipeline æ˜¯æœ€ç®€å•çš„ä½¿ç”¨æ–¹å¼ï¼Œå¼€ç®±å³ç”¨ã€‚

### åŸºæœ¬ç”¨æ³•

```python
from transformers import pipeline

# è‡ªåŠ¨ä¸‹è½½æ¨¡å‹å¹¶æ¨ç†
classifier = pipeline("sentiment-analysis")
result = classifier("I love Hugging Face!")
# [{'label': 'POSITIVE', 'score': 0.9998}]
```

### å¸¸ç”¨ Pipeline ä»»åŠ¡

| ä»»åŠ¡åç§°                       | æè¿°         | ç¤ºä¾‹                                       |
| ------------------------------ | ------------ | ------------------------------------------ |
| `sentiment-analysis`           | æƒ…æ„Ÿåˆ†æ     | `pipeline("sentiment-analysis")`           |
| `text-classification`          | æ–‡æœ¬åˆ†ç±»     | `pipeline("text-classification")`          |
| `ner`                          | å‘½åå®ä½“è¯†åˆ« | `pipeline("ner")`                          |
| `question-answering`           | é—®ç­”         | `pipeline("question-answering")`           |
| `summarization`                | æ–‡æœ¬æ‘˜è¦     | `pipeline("summarization")`                |
| `translation`                  | ç¿»è¯‘         | `pipeline("translation_en_to_fr")`         |
| `text-generation`              | æ–‡æœ¬ç”Ÿæˆ     | `pipeline("text-generation")`              |
| `fill-mask`                    | å¡«ç©º         | `pipeline("fill-mask")`                    |
| `zero-shot-classification`     | é›¶æ ·æœ¬åˆ†ç±»   | `pipeline("zero-shot-classification")`     |
| `image-classification`         | å›¾åƒåˆ†ç±»     | `pipeline("image-classification")`         |
| `object-detection`             | ç›®æ ‡æ£€æµ‹     | `pipeline("object-detection")`             |
| `automatic-speech-recognition` | è¯­éŸ³è¯†åˆ«     | `pipeline("automatic-speech-recognition")` |

### Pipeline è¯¦ç»†ç¤ºä¾‹

```python
from transformers import pipeline

# 1. æ–‡æœ¬ç”Ÿæˆ
generator = pipeline("text-generation", model="gpt2")
result = generator("Hello, I'm a language model", max_length=50, num_return_sequences=2)

# 2. é—®ç­”ç³»ç»Ÿ
qa = pipeline("question-answering")
result = qa(
    question="What is Hugging Face?",
    context="Hugging Face is a company that provides NLP tools."
)

# 3. å‘½åå®ä½“è¯†åˆ«
ner = pipeline("ner", aggregation_strategy="simple")
result = ner("My name is John and I live in New York")

# 4. æ–‡æœ¬æ‘˜è¦
summarizer = pipeline("summarization", model="facebook/bart-large-cnn")
result = summarizer(long_text, max_length=130, min_length=30)

# 5. ç¿»è¯‘
translator = pipeline("translation", model="Helsinki-NLP/opus-mt-en-zh")
result = translator("Hello, how are you?")

# 6. é›¶æ ·æœ¬åˆ†ç±»
classifier = pipeline("zero-shot-classification")
result = classifier(
    "This is a course about Python programming",
    candidate_labels=["education", "politics", "business"]
)

# 7. å›¾åƒåˆ†ç±»
image_classifier = pipeline("image-classification")
result = image_classifier("path/to/image.jpg")

# 8. è¯­éŸ³è¯†åˆ«
asr = pipeline("automatic-speech-recognition", model="openai/whisper-base")
result = asr("path/to/audio.mp3")
```

### æŒ‡å®šè®¾å¤‡

```python
# ä½¿ç”¨ GPU
pipe = pipeline("text-classification", device=0)  # ç¬¬ä¸€å— GPU
pipe = pipeline("text-classification", device="cuda:0")

# ä½¿ç”¨ CPU
pipe = pipeline("text-classification", device=-1)
pipe = pipeline("text-classification", device="cpu")

# è‡ªåŠ¨é€‰æ‹©
pipe = pipeline("text-classification", device_map="auto")
```

---

## 3. Tokenizer åˆ†è¯å™¨

### åŠ è½½ Tokenizer

```python
from transformers import AutoTokenizer

# ä» Hub åŠ è½½
tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")

# ä»æœ¬åœ°åŠ è½½
tokenizer = AutoTokenizer.from_pretrained("./my_model/")
```

### åŸºæœ¬ç¼–ç /è§£ç 

```python
# ç¼–ç 
text = "Hello, how are you?"
encoded = tokenizer(text)
# {'input_ids': [101, 7592, ...], 'attention_mask': [1, 1, ...], 'token_type_ids': [...]}

# ä»…è·å– token ids
ids = tokenizer.encode(text)

# è§£ç 
decoded = tokenizer.decode(ids)

# æ‰¹é‡ç¼–ç 
texts = ["Hello world", "How are you"]
batch_encoded = tokenizer(texts)
```

### é«˜çº§ç¼–ç é€‰é¡¹

```python
# å¡«å……å’Œæˆªæ–­
encoded = tokenizer(
    text,
    padding="max_length",       # å¡«å……åˆ°æœ€å¤§é•¿åº¦
    truncation=True,            # å¯ç”¨æˆªæ–­
    max_length=512,             # æœ€å¤§é•¿åº¦
    return_tensors="pt",        # è¿”å› PyTorch å¼ é‡ ("tf" for TensorFlow)
    add_special_tokens=True,    # æ·»åŠ ç‰¹æ®Š token
)

# æ‰¹é‡å¤„ç†ï¼ˆåŠ¨æ€å¡«å……ï¼‰
encoded = tokenizer(
    texts,
    padding=True,               # å¡«å……åˆ°æ‰¹æ¬¡æœ€é•¿
    truncation=True,
    return_tensors="pt"
)
```

### å¡«å……é€‰é¡¹

| å‚æ•°                   | è¯´æ˜               |
| ---------------------- | ------------------ |
| `padding=True`         | å¡«å……åˆ°æ‰¹æ¬¡æœ€é•¿åºåˆ— |
| `padding="max_length"` | å¡«å……åˆ° max_length  |
| `padding="longest"`    | åŒ True            |
| `padding=False`        | ä¸å¡«å……             |

### è¿”å›å¼ é‡ç±»å‹

| å‚æ•°                  | è¯´æ˜              |
| --------------------- | ----------------- |
| `return_tensors="pt"` | PyTorch tensor    |
| `return_tensors="tf"` | TensorFlow tensor |
| `return_tensors="np"` | NumPy array       |
| ä¸è®¾ç½®                | Python list       |

### Tokenizer å¸¸ç”¨å±æ€§å’Œæ–¹æ³•

```python
# è¯æ±‡è¡¨å¤§å°
tokenizer.vocab_size

# ç‰¹æ®Š token
tokenizer.pad_token        # [PAD]
tokenizer.unk_token        # [UNK]
tokenizer.cls_token        # [CLS]
tokenizer.sep_token        # [SEP]
tokenizer.mask_token       # [MASK]

# ç‰¹æ®Š token ID
tokenizer.pad_token_id
tokenizer.unk_token_id

# å°† tokens è½¬æ¢ä¸º ids
tokenizer.convert_tokens_to_ids(["hello", "world"])

# å°† ids è½¬æ¢ä¸º tokens
tokenizer.convert_ids_to_tokens([101, 7592])

# åˆ†è¯ï¼ˆä¸è½¬æ¢ä¸º idsï¼‰
tokenizer.tokenize("Hello world")  # ['hello', 'world']
```

---

## 4. æ¨¡å‹åŠ è½½ä¸ä½¿ç”¨

### è‡ªåŠ¨åŠ è½½æ¨¡å‹

```python
from transformers import AutoModel, AutoModelForSequenceClassification
from transformers import AutoModelForCausalLM, AutoModelForSeq2SeqLM

# åŠ è½½åŸºç¡€æ¨¡å‹
model = AutoModel.from_pretrained("bert-base-uncased")

# åŠ è½½ç‰¹å®šä»»åŠ¡æ¨¡å‹
model = AutoModelForSequenceClassification.from_pretrained("bert-base-uncased")
model = AutoModelForCausalLM.from_pretrained("gpt2")
model = AutoModelForSeq2SeqLM.from_pretrained("t5-base")
```

### å¸¸ç”¨ AutoModel ç±»

| ç±»å                                 | ç”¨é€”                   |
| ------------------------------------ | ---------------------- |
| `AutoModel`                          | åŸºç¡€æ¨¡å‹ï¼ˆæ— ä»»åŠ¡å¤´ï¼‰   |
| `AutoModelForSequenceClassification` | æ–‡æœ¬åˆ†ç±»               |
| `AutoModelForTokenClassification`    | Token åˆ†ç±» (NER)       |
| `AutoModelForQuestionAnswering`      | é—®ç­”                   |
| `AutoModelForCausalLM`               | å› æœè¯­è¨€æ¨¡å‹ï¼ˆGPTç±»ï¼‰  |
| `AutoModelForSeq2SeqLM`              | Seq2Seqï¼ˆT5, BARTç±»ï¼‰  |
| `AutoModelForMaskedLM`               | æ©ç è¯­è¨€æ¨¡å‹ï¼ˆBERTç±»ï¼‰ |
| `AutoModelForImageClassification`    | å›¾åƒåˆ†ç±»               |
| `AutoModelForObjectDetection`        | ç›®æ ‡æ£€æµ‹               |
| `AutoModelForSpeechSeq2Seq`          | è¯­éŸ³è½¬æ–‡æœ¬             |

### æ¨¡å‹æ¨ç†

```python
import torch
from transformers import AutoTokenizer, AutoModelForSequenceClassification

tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")
model = AutoModelForSequenceClassification.from_pretrained("bert-base-uncased")

# ç¼–ç è¾“å…¥
inputs = tokenizer("Hello world", return_tensors="pt")

# æ¨ç†
with torch.no_grad():
    outputs = model(**inputs)

# è·å– logits
logits = outputs.logits
predictions = torch.argmax(logits, dim=-1)
```

### æ–‡æœ¬ç”Ÿæˆ

```python
from transformers import AutoTokenizer, AutoModelForCausalLM

tokenizer = AutoTokenizer.from_pretrained("gpt2")
model = AutoModelForCausalLM.from_pretrained("gpt2")

inputs = tokenizer("Hello, I'm a", return_tensors="pt")

# ç”Ÿæˆæ–‡æœ¬
outputs = model.generate(
    **inputs,
    max_length=50,
    num_return_sequences=1,
    do_sample=True,
    temperature=0.7,
    top_k=50,
    top_p=0.95,
    pad_token_id=tokenizer.eos_token_id
)

generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
```

### ç”Ÿæˆå‚æ•°è¯¦è§£

```python
model.generate(
    inputs,
    # é•¿åº¦æ§åˆ¶
    max_length=100,              # æœ€å¤§ç”Ÿæˆé•¿åº¦
    max_new_tokens=50,           # æœ€å¤§æ–°ç”Ÿæˆ token æ•°
    min_length=10,               # æœ€å°é•¿åº¦

    # é‡‡æ ·ç­–ç•¥
    do_sample=True,              # æ˜¯å¦é‡‡æ ·
    temperature=0.7,             # æ¸©åº¦ï¼ˆè¶Šé«˜è¶Šéšæœºï¼‰
    top_k=50,                    # Top-K é‡‡æ ·
    top_p=0.95,                  # Top-P (nucleus) é‡‡æ ·

    # Beam Search
    num_beams=5,                 # Beam æ•°é‡
    early_stopping=True,         # æ—©åœ
    no_repeat_ngram_size=2,      # é¿å…é‡å¤ n-gram

    # å…¶ä»–
    num_return_sequences=3,      # è¿”å›åºåˆ—æ•°
    pad_token_id=tokenizer.pad_token_id,
    eos_token_id=tokenizer.eos_token_id,
)
```

---

## 5. Datasets æ•°æ®é›†

### å®‰è£…

```bash
pip install datasets
```

### åŠ è½½æ•°æ®é›†

```python
from datasets import load_dataset

# ä» Hub åŠ è½½
dataset = load_dataset("imdb")
dataset = load_dataset("glue", "mrpc")  # åŠ è½½å­é›†

# åŠ è½½ç‰¹å®šåˆ†å‰²
train_dataset = load_dataset("imdb", split="train")
test_dataset = load_dataset("imdb", split="test[:100]")  # å‰100æ¡

# ä»æœ¬åœ°æ–‡ä»¶åŠ è½½
dataset = load_dataset("csv", data_files="data.csv")
dataset = load_dataset("json", data_files="data.json")
dataset = load_dataset("text", data_files="data.txt")

# ä»å­—å…¸åˆ›å»º
from datasets import Dataset
data = {"text": ["Hello", "World"], "label": [0, 1]}
dataset = Dataset.from_dict(data)

# ä» pandas åˆ›å»º
dataset = Dataset.from_pandas(df)
```

### æ•°æ®é›†æ“ä½œ

```python
# æŸ¥çœ‹æ•°æ®é›†ä¿¡æ¯
print(dataset)
print(dataset.features)
print(dataset.column_names)

# è®¿é—®æ•°æ®
print(dataset[0])                    # ç¬¬ä¸€æ¡
print(dataset["text"])               # æ•´åˆ—
print(dataset[0:5])                  # åˆ‡ç‰‡

# åˆ†å‰²æ•°æ®é›†
dataset = dataset.train_test_split(test_size=0.2)

# æ‰“ä¹±
dataset = dataset.shuffle(seed=42)

# é€‰æ‹©/è¿‡æ»¤
dataset = dataset.select(range(100))  # é€‰æ‹©å‰100æ¡
dataset = dataset.filter(lambda x: len(x["text"]) > 10)

# æ˜ å°„ï¼ˆé¢„å¤„ç†ï¼‰
def preprocess(examples):
    return tokenizer(examples["text"], truncation=True, padding=True)

dataset = dataset.map(preprocess, batched=True)

# é‡å‘½å/åˆ é™¤åˆ—
dataset = dataset.rename_column("text", "input_text")
dataset = dataset.remove_columns(["unnecessary_column"])

# è®¾ç½®æ ¼å¼
dataset.set_format("torch", columns=["input_ids", "attention_mask", "label"])
```

### å¸¸ç”¨å…¬å¼€æ•°æ®é›†

| æ•°æ®é›†       | åŠ è½½æ–¹å¼                             | æè¿°             |
| ------------ | ------------------------------------ | ---------------- |
| IMDb         | `load_dataset("imdb")`               | ç”µå½±è¯„è®ºæƒ…æ„Ÿåˆ†æ |
| GLUE         | `load_dataset("glue", "sst2")`       | NLU åŸºå‡†æµ‹è¯•     |
| SQuAD        | `load_dataset("squad")`              | é—®ç­”æ•°æ®é›†       |
| CoNLL-2003   | `load_dataset("conll2003")`          | NER æ•°æ®é›†       |
| WMT          | `load_dataset("wmt16", "de-en")`     | ç¿»è¯‘æ•°æ®é›†       |
| Common Voice | `load_dataset("common_voice", "en")` | è¯­éŸ³æ•°æ®é›†       |
| ImageNet     | `load_dataset("imagenet-1k")`        | å›¾åƒåˆ†ç±»         |

---

## 6. æ¨¡å‹è®­ç»ƒä¸å¾®è°ƒ

### ä½¿ç”¨ Trainerï¼ˆæ¨èï¼‰

```python
from transformers import (
    AutoTokenizer,
    AutoModelForSequenceClassification,
    Trainer,
    TrainingArguments,
    DataCollatorWithPadding
)
from datasets import load_dataset
import evaluate
import numpy as np

# 1. åŠ è½½æ•°æ®
dataset = load_dataset("imdb")

# 2. åŠ è½½ tokenizer å’Œæ¨¡å‹
tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")
model = AutoModelForSequenceClassification.from_pretrained(
    "bert-base-uncased",
    num_labels=2
)

# 3. æ•°æ®é¢„å¤„ç†
def preprocess_function(examples):
    return tokenizer(examples["text"], truncation=True, max_length=512)

tokenized_datasets = dataset.map(preprocess_function, batched=True)

# 4. æ•°æ®æ•´ç†å™¨
data_collator = DataCollatorWithPadding(tokenizer=tokenizer)

# 5. è¯„ä¼°æŒ‡æ ‡
accuracy = evaluate.load("accuracy")

def compute_metrics(eval_pred):
    predictions, labels = eval_pred
    predictions = np.argmax(predictions, axis=1)
    return accuracy.compute(predictions=predictions, references=labels)

# 6. è®­ç»ƒå‚æ•°
training_args = TrainingArguments(
    output_dir="./results",
    learning_rate=2e-5,
    per_device_train_batch_size=16,
    per_device_eval_batch_size=16,
    num_train_epochs=3,
    weight_decay=0.01,
    evaluation_strategy="epoch",
    save_strategy="epoch",
    load_best_model_at_end=True,
    push_to_hub=False,
)

# 7. åˆ›å»º Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_datasets["train"],
    eval_dataset=tokenized_datasets["test"],
    tokenizer=tokenizer,
    data_collator=data_collator,
    compute_metrics=compute_metrics,
)

# 8. è®­ç»ƒ
trainer.train()

# 9. è¯„ä¼°
trainer.evaluate()

# 10. ä¿å­˜æ¨¡å‹
trainer.save_model("./my_model")
```

### TrainingArguments å¸¸ç”¨å‚æ•°

```python
training_args = TrainingArguments(
    # è¾“å‡ºè®¾ç½®
    output_dir="./results",
    overwrite_output_dir=True,

    # è®­ç»ƒè¶…å‚æ•°
    num_train_epochs=3,
    per_device_train_batch_size=16,
    per_device_eval_batch_size=32,
    gradient_accumulation_steps=2,    # æ¢¯åº¦ç´¯ç§¯
    learning_rate=5e-5,
    weight_decay=0.01,
    warmup_steps=500,
    warmup_ratio=0.1,

    # è¯„ä¼°å’Œä¿å­˜
    evaluation_strategy="steps",       # "no", "steps", "epoch"
    eval_steps=500,
    save_strategy="steps",
    save_steps=500,
    save_total_limit=3,               # æœ€å¤šä¿å­˜æ¨¡å‹æ•°
    load_best_model_at_end=True,
    metric_for_best_model="accuracy",

    # æ—¥å¿—
    logging_dir="./logs",
    logging_steps=100,
    report_to="tensorboard",          # "wandb", "tensorboard", "none"

    # ç¡¬ä»¶
    fp16=True,                        # æ··åˆç²¾åº¦è®­ç»ƒ
    bf16=False,
    dataloader_num_workers=4,

    # Hub
    push_to_hub=False,
    hub_model_id="my-model",
)
```

### ä½¿ç”¨åŸç”Ÿ PyTorch è®­ç»ƒ

```python
import torch
from torch.utils.data import DataLoader
from transformers import AutoTokenizer, AutoModelForSequenceClassification
from transformers import AdamW, get_scheduler

# å‡†å¤‡æ•°æ®
tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")
model = AutoModelForSequenceClassification.from_pretrained("bert-base-uncased")

# åˆ›å»º DataLoader
train_dataloader = DataLoader(tokenized_dataset, shuffle=True, batch_size=8)

# ä¼˜åŒ–å™¨å’Œå­¦ä¹ ç‡è°ƒåº¦å™¨
optimizer = AdamW(model.parameters(), lr=5e-5)
num_training_steps = len(train_dataloader) * num_epochs
lr_scheduler = get_scheduler(
    "linear",
    optimizer=optimizer,
    num_warmup_steps=0,
    num_training_steps=num_training_steps
)

# è®­ç»ƒå¾ªç¯
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)

model.train()
for epoch in range(num_epochs):
    for batch in train_dataloader:
        batch = {k: v.to(device) for k, v in batch.items()}
        outputs = model(**batch)
        loss = outputs.loss
        loss.backward()

        optimizer.step()
        lr_scheduler.step()
        optimizer.zero_grad()
```

### ä½¿ç”¨ Accelerate åˆ†å¸ƒå¼è®­ç»ƒ

```python
from accelerate import Accelerator

accelerator = Accelerator()

model, optimizer, train_dataloader, lr_scheduler = accelerator.prepare(
    model, optimizer, train_dataloader, lr_scheduler
)

for epoch in range(num_epochs):
    for batch in train_dataloader:
        outputs = model(**batch)
        loss = outputs.loss
        accelerator.backward(loss)
        optimizer.step()
        lr_scheduler.step()
        optimizer.zero_grad()
```

---

## 7. æ¨¡å‹ä¿å­˜ä¸ä¸Šä¼ 

### æœ¬åœ°ä¿å­˜å’ŒåŠ è½½

```python
# ä¿å­˜
model.save_pretrained("./my_model")
tokenizer.save_pretrained("./my_model")

# åŠ è½½
model = AutoModel.from_pretrained("./my_model")
tokenizer = AutoTokenizer.from_pretrained("./my_model")
```

### ä¸Šä¼ åˆ° Hub

```python
# æ–¹æ³•1: ä½¿ç”¨ CLI ç™»å½•
# huggingface-cli login

# æ–¹æ³•2: åœ¨ä»£ç ä¸­ç™»å½•
from huggingface_hub import login
login(token="your_token")

# ä¸Šä¼ æ¨¡å‹
model.push_to_hub("your-username/model-name")
tokenizer.push_to_hub("your-username/model-name")

# é€šè¿‡ Trainer ä¸Šä¼ 
trainer.push_to_hub()
```

### Hub API ä½¿ç”¨

```python
from huggingface_hub import HfApi, hf_hub_download

api = HfApi()

# åˆ—å‡ºæ¨¡å‹
models = api.list_models(filter="text-classification")

# ä¸‹è½½æ–‡ä»¶
hf_hub_download(repo_id="bert-base-uncased", filename="config.json")

# åˆ›å»ºä»“åº“
api.create_repo(repo_id="my-new-model")

# ä¸Šä¼ æ–‡ä»¶
api.upload_file(
    path_or_fileobj="./model.bin",
    path_in_repo="model.bin",
    repo_id="username/my-model"
)
```

---

## 8. å¸¸ç”¨æ¨¡å‹é€ŸæŸ¥

### NLP æ¨¡å‹

| æ¨¡å‹       | ç”¨é€”             | åŠ è½½æ–¹å¼                  |
| ---------- | ---------------- | ------------------------- |
| BERT       | ç¼–ç å™¨ï¼Œåˆ†ç±»/NER | `bert-base-uncased`       |
| RoBERTa    | ä¼˜åŒ–çš„ BERT      | `roberta-base`            |
| DistilBERT | è½»é‡ BERT        | `distilbert-base-uncased` |
| ALBERT     | è½»é‡ BERT        | `albert-base-v2`          |
| GPT-2      | æ–‡æœ¬ç”Ÿæˆ         | `gpt2`, `gpt2-medium`     |
| GPT-Neo    | å¼€æº GPT         | `EleutherAI/gpt-neo-1.3B` |
| LLaMA      | Meta å¤§æ¨¡å‹      | `meta-llama/Llama-2-7b`   |
| T5         | Seq2Seq          | `t5-base`, `t5-large`     |
| BART       | Seq2Seqï¼Œæ‘˜è¦    | `facebook/bart-large-cnn` |
| XLNet      | è‡ªå›å½’+è‡ªç¼–ç     | `xlnet-base-cased`        |

### ä¸­æ–‡æ¨¡å‹

| æ¨¡å‹         | åŠ è½½æ–¹å¼                      |
| ------------ | ----------------------------- |
| BERT ä¸­æ–‡    | `bert-base-chinese`           |
| MacBERT      | `hfl/chinese-macbert-base`    |
| RoBERTa ä¸­æ–‡ | `hfl/chinese-roberta-wwm-ext` |
| ERNIE        | `nghuyong/ernie-3.0-base-zh`  |
| ChatGLM      | `THUDM/chatglm3-6b`           |
| Qwen         | `Qwen/Qwen-7B`                |

### å¤šæ¨¡æ€æ¨¡å‹

| æ¨¡å‹    | ç”¨é€”     | åŠ è½½æ–¹å¼                                |
| ------- | -------- | --------------------------------------- |
| CLIP    | å›¾æ–‡åŒ¹é… | `openai/clip-vit-base-patch32`          |
| ViT     | å›¾åƒåˆ†ç±» | `google/vit-base-patch16-224`           |
| BLIP    | å›¾åƒæè¿° | `Salesforce/blip-image-captioning-base` |
| Whisper | è¯­éŸ³è¯†åˆ« | `openai/whisper-base`                   |

---

## 9. é«˜çº§é…ç½®

### åŠ è½½å¤§æ¨¡å‹ï¼ˆå†…å­˜ä¼˜åŒ–ï¼‰

```python
from transformers import AutoModelForCausalLM

# 8ä½é‡åŒ–
model = AutoModelForCausalLM.from_pretrained(
    "bigscience/bloom-7b1",
    load_in_8bit=True,
    device_map="auto"
)

# 4ä½é‡åŒ–
model = AutoModelForCausalLM.from_pretrained(
    "meta-llama/Llama-2-7b",
    load_in_4bit=True,
    device_map="auto"
)

# æŒ‡å®šè®¾å¤‡æ˜ å°„
model = AutoModelForCausalLM.from_pretrained(
    "gpt2",
    device_map="auto",              # è‡ªåŠ¨åˆ†é…
    torch_dtype=torch.float16,       # åŠç²¾åº¦
    low_cpu_mem_usage=True
)

# ä½¿ç”¨ offload
model = AutoModelForCausalLM.from_pretrained(
    "bigscience/bloom",
    device_map="auto",
    offload_folder="offload",        # CPU/ç£ç›˜ offload
)
```

### PEFT (å‚æ•°é«˜æ•ˆå¾®è°ƒ)

```bash
pip install peft
```

```python
from peft import LoraConfig, get_peft_model, TaskType

# LoRA é…ç½®
lora_config = LoraConfig(
    task_type=TaskType.CAUSAL_LM,
    r=8,                              # LoRA rank
    lora_alpha=32,
    lora_dropout=0.1,
    target_modules=["q_proj", "v_proj"]
)

# åº”ç”¨ LoRA
model = get_peft_model(model, lora_config)
model.print_trainable_parameters()

# ä¿å­˜ LoRA æƒé‡
model.save_pretrained("./lora_model")

# åŠ è½½ LoRA æƒé‡
from peft import PeftModel
base_model = AutoModelForCausalLM.from_pretrained("base_model")
model = PeftModel.from_pretrained(base_model, "./lora_model")
```

### æ¢¯åº¦æ£€æŸ¥ç‚¹

```python
model.gradient_checkpointing_enable()
```

### Flash Attention

```python
model = AutoModelForCausalLM.from_pretrained(
    "model_name",
    attn_implementation="flash_attention_2",
    torch_dtype=torch.float16
)
```

---

## 10. å¸¸è§é—®é¢˜è§£å†³

### å†…å­˜ä¸è¶³ (OOM)

```python
# 1. å‡å° batch size
# 2. ä½¿ç”¨æ¢¯åº¦ç´¯ç§¯
training_args = TrainingArguments(
    gradient_accumulation_steps=4,
    per_device_train_batch_size=4,  # æœ‰æ•ˆ batch = 4 * 4 = 16
)

# 3. ä½¿ç”¨æ··åˆç²¾åº¦
training_args = TrainingArguments(fp16=True)

# 4. ä½¿ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹
model.gradient_checkpointing_enable()

# 5. é‡åŒ–åŠ è½½
model = AutoModel.from_pretrained("model", load_in_8bit=True)
```

### ä¸‹è½½é—®é¢˜

```python
# è®¾ç½®é•œåƒæºï¼ˆä¸­å›½å¤§é™†ç”¨æˆ·ï¼‰
import os
os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'

# è®¾ç½®ç¼“å­˜ç›®å½•
os.environ['HF_HOME'] = '/path/to/cache'
os.environ['TRANSFORMERS_CACHE'] = '/path/to/cache'

# ç¦»çº¿æ¨¡å¼
os.environ['TRANSFORMERS_OFFLINE'] = '1'
model = AutoModel.from_pretrained("./local_model")
```

### è­¦å‘Šå¤„ç†

```python
# å¿½ç•¥è­¦å‘Š
import warnings
warnings.filterwarnings("ignore")

# è®¾ç½®æ—¥å¿—çº§åˆ«
import logging
logging.set_verbosity_error()

from transformers import logging
logging.set_verbosity_warning()
```

### å¸¸ç”¨ç¯å¢ƒå˜é‡

| ç¯å¢ƒå˜é‡               | è¯´æ˜                    |
| ---------------------- | ----------------------- |
| `HF_HOME`              | Hugging Face ç¼“å­˜æ ¹ç›®å½• |
| `HF_TOKEN`             | API Token               |
| `TRANSFORMERS_CACHE`   | æ¨¡å‹ç¼“å­˜ç›®å½•            |
| `HF_DATASETS_CACHE`    | æ•°æ®é›†ç¼“å­˜ç›®å½•          |
| `TRANSFORMERS_OFFLINE` | ç¦»çº¿æ¨¡å¼                |
| `HF_ENDPOINT`          | Hub ç«¯ç‚¹ URL            |

---

## å¿«é€Ÿå‚è€ƒå¡ç‰‡

```python
# ğŸš€ æœ€ç®€ä½¿ç”¨
from transformers import pipeline
pipe = pipeline("task-name")
result = pipe("input text")

# ğŸ“¦ æ ‡å‡†ä½¿ç”¨
from transformers import AutoTokenizer, AutoModel
tokenizer = AutoTokenizer.from_pretrained("model-name")
model = AutoModel.from_pretrained("model-name")
inputs = tokenizer("text", return_tensors="pt")
outputs = model(**inputs)

# ğŸ¯ è®­ç»ƒæµç¨‹
from transformers import Trainer, TrainingArguments
args = TrainingArguments(output_dir="./output", ...)
trainer = Trainer(model=model, args=args, train_dataset=dataset, ...)
trainer.train()
trainer.save_model()
```

---

> ğŸ“š **å®˜æ–¹æ–‡æ¡£**: https://huggingface.co/docs/transformers
>
> ğŸ¤— **æ¨¡å‹ä¸­å¿ƒ**: https://huggingface.co/models
>
> ğŸ“Š **æ•°æ®é›†ä¸­å¿ƒ**: https://huggingface.co/datasets
